---
title: "ASSESSED HOMEWORK"
author: "George, Jack and Lola"
date: "19/04/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r cars}
#loading libraries and data 
library("gamlss.dist")
library(caret)
library(ggplot2)
library(cvms)
library(groupdata2)   
library(dplyr)
library(knitr)         
library(e1071)
library(tidyverse)
library(DescTools)
library(MASS)

load(file = url("https://mfasiolo.github.io/TOI/gryphus2.RData"))
(gryphus)
```

# Q1)
```{r}
calculate_p <- function(theta, df){
  P <- length(theta) - 1
  heta <- rep(theta[1], nrow(df))
  for(degree in 1:P){
    heta <- heta + theta[degree+1]*(df$dens - 15)^degree
  }
  
  p <- exp(heta)/(exp(heta)+1)
  # in case inf/inf -> NaN
  p[is.nan(p)] <- 1
  
  return(p)
}

ll_poly_logistic <- function(theta, df){
  
  P <- length(theta) - 1
  
  # Sets initial value of log-likelihood to zero
  L <- 0
  
  p <- calculate_p(theta, df)
  
  # Iterates through each row adding the probability of the outcome (0 or 1) to the log-likelihood
  for (i in 1:nrow(df)){
    if (df[i,2] == 1){
      L <- L + log(p[i] + 1e-6)
    } else {
      L <- L + log(1-p[i] + 1e-6)
    }
  }
  
  return(-L)
}
```

# Q2)

```{r}
optim_poly_logi <- function(df, theta0){
  res <- optim(
    ll_poly_logistic,
    par=theta0,
    df=df,
    method="BFGS",
    hessian=T,
    control=list(maxit=500)
  )
  if(res$convergence == 0){
    return(res)
  }else{
    print("Error!")
    return()
  }
}
```



```{r}
# MLE with full data result
# P = 1
theta0 <- c(0, 0)
parameters1 <- optim_poly_logi(gryphus, theta0)
# P = 2
parameters2 <- optim_poly_logi(gryphus, c(parameters1$par, 0))
# P = 3
parameters3 <- optim_poly_logi(gryphus, c(parameters2$par, 0))
# P = 4
parameters4 <- optim_poly_logi(gryphus, c(parameters3$par, 0))
# P = 5
parameters5 <- optim_poly_logi(gryphus, c(parameters4$par, 0))
# P = 6
parameters6 <- optim_poly_logi(gryphus, c(parameters5$par, 0))
# P = 7
parameters7 <- optim_poly_logi(gryphus, c(parameters6$par, 0))
#test
#gryphus$dens2 <- gryphus$dens - 15
#mod <- glm(surv ~ dens2, data = gryphus, family = binomial)
```

```{r}
pred1 <- calculate_p(parameters1$par, gryphus)
pred2 <- calculate_p(parameters2$par, gryphus)
pred3 <- calculate_p(parameters3$par, gryphus)
pred4 <- calculate_p(parameters4$par, gryphus)
pred5 <- calculate_p(parameters5$par, gryphus)
pred6 <- calculate_p(parameters6$par, gryphus)

data <- cbind(gryphus, pred1, pred2, pred3, pred4, pred5, pred6)

data <- rbind(gryphus, gryphus, gryphus, gryphus, gryphus, gryphus)
preds <- c(pred1, pred2, pred3, pred4, pred5, pred6)
data <- cbind(data, preds)
data$cat <- as.factor(sort(rep(1:6, nrow(gryphus))))

ggplot(data) +
  geom_boxplot(aes(group=cat, y=preds))
```
# Q3)

```{r}
plot_CI <-function(pars, df, title) {
  pred <- calculate_p(pars$par, df)
  #scatter.df <- data.frame(df, pred)
  
  dens <- seq(from=8, to=22.5, by=0.01)
  x.df <- data.frame(dens)
  probs1 <- calculate_p(pars$par, x.df)
  line.df <- data.frame(dens, probs1)

  X <- rep(1, nrow(x.df))
  for(i in 1:(length(pars$par)-1)){
    X <- cbind(X, (dens-15)^i)
  }
  sigma <- solve(pars$hessian)
  est_var <- diag(X%*%sigma%*%t(X))
  est_heta <- cbind(X%*%pars$par - 1.96*sqrt(est_var),
                X%*%pars$par + 1.96*sqrt(est_var))
  est_ci <- exp(est_heta)/(1+exp(est_heta))
  
  li <- est_ci[,1]
  ui <- est_ci[,2]
  ui[is.nan(ui)] <- 1
  CI.df <- data.frame(x.df$dens, li, ui)

  ggplot() + geom_point(data = df, aes(x = dens, y = surv, color = factor(surv))) + geom_line(data = CI.df, aes(x = x.df.dens, y = li), color = "grey70") + geom_line(data = CI.df, aes(x = x.df.dens, y = ui), color = "grey70") +
    geom_ribbon(data = CI.df, aes(ymin = li, ymax = ui, x= x.df.dens), fill = "grey70", alpha = 0.3) + geom_line(data = line.df, aes(x = dens, y = probs1)) +
    labs(
        x = expression("Density of Lemmings (No. of Lemmings per km"^2*")"),
        y = "Predicted Probability of Survival",
        color = "Survived",
        title = title,
        #subtitle = "subtitle",
        #caption = "caption"
    ) + 
    geom_hline(aes(yintercept=.5), color="gray", linetype="longdash") + xlim(8, 22.5)
}
```

```{r}
plot_CI(parameters1, gryphus, title = "Polynomial Model with P=1")
plot_CI(parameters2, gryphus, title = "Polynomial Model with P=2")
plot_CI(parameters3, gryphus, title = "Polynomial Model with P=3")
plot_CI(parameters4, gryphus, title = "Polynomial Model with P=4")
plot_CI(parameters5, gryphus, title = "Polynomial Model with P=5")
plot_CI(parameters6, gryphus, title = "Polynomial Model with P=6")
```
# Q4)

##Plotting all the models
```{r}
plot_all <-function() {
  dens <- seq(from=8, to=22.5, by=0.01)
  x.df <- data.frame(dens)
  
  probs1 <- calculate_p(parameters1$par, x.df)
  probs2 <- calculate_p(parameters2$par, x.df)
  probs3 <- calculate_p(parameters3$par, x.df)
  probs4 <- calculate_p(parameters4$par, x.df)
  probs5 <- calculate_p(parameters5$par, x.df)
  probs6 <- calculate_p(parameters6$par, x.df)
  
  line.df <- data.frame(dens, probs1, probs2, probs3, probs4, probs5, probs6)
  
  ggplot() + geom_line(data = line.df, aes(x = dens, y = probs1, color= "P=1")) + geom_line(data = line.df, aes(x = dens, y = probs2, color= "P=2")) + geom_line(data = line.df, aes(x = dens, y = probs3, color= "P=3")) + geom_line(data = line.df, aes(x = dens, y = probs4, color= "P=4")) + geom_line(data = line.df, aes(x = dens, y = probs5, color= "P=5")) + geom_line(data = line.df, aes(x = dens, y = probs6, color= "P=6")) + 
    labs(
        x = expression("Density of Lemmings (No. of Lemmings per km"^2*")"),
        y = "Predicted Probability of Survival",
        color = "Model",
        title = "Overlapping Models",
        #subtitle = "subtitle",
        #caption = "caption"
    ) + 
    geom_hline(aes(yintercept=.5), color="gray", linetype="longdash") + xlim(8, 23)
  
}

plot_all()
```
##Using cross-validation to check the accuracies of the polynomial models
```{r}
cross_val <- function(data, params0, cv=10,repeats=5){
  accuracies <- c()
  for(i in 1:repeats){
    len <- floor(nrow(data)/cv)
    idxs <- sample(nrow(data), nrow(data))
    for(j in 1:cv){
      val_idxs <- idxs[(len*(j-1)):(len*j)]
      train_idxs <- idxs[-c((len*(j-1)):(len*j))]
      res <- optim_poly_logi(data[train_idxs,], params0)
      pred <- calculate_p(res$par, data[val_idxs,])>=0.5
      acc <- sum(pred==data[val_idxs,"surv"])/length(pred)
      accuracies <- append(accuracies, acc)
    }
  }
  return(accuracies)
}
```

```{r}
set.seed(2)
three_acc1 <- cross_val(gryphus, parameters1$par, cv=3, repeats=10)
three_acc2 <- cross_val(gryphus, parameters2$par, cv=3, repeats=10)
three_acc3 <- cross_val(gryphus, parameters3$par, cv=3, repeats=10)
three_acc4 <- cross_val(gryphus, parameters4$par, cv=3, repeats=10)
three_acc5 <- cross_val(gryphus, parameters5$par, cv=3, repeats=10)
three_acc6 <- cross_val(gryphus, parameters6$par, cv=3, repeats=10)

seven_acc1 <- cross_val(gryphus, parameters1$par, cv=7, repeats=10)
seven_acc2 <- cross_val(gryphus, parameters2$par, cv=7, repeats=10)
seven_acc3 <- cross_val(gryphus, parameters3$par, cv=7, repeats=10)
seven_acc4 <- cross_val(gryphus, parameters4$par, cv=7, repeats=10)
seven_acc5 <- cross_val(gryphus, parameters5$par, cv=7, repeats=10)
seven_acc6 <- cross_val(gryphus, parameters6$par, cv=7, repeats=10)

nine_acc1 <- cross_val(gryphus, parameters1$par, cv=9, repeats=10)
nine_acc2 <- cross_val(gryphus, parameters2$par, cv=9, repeats=10)
nine_acc3 <- cross_val(gryphus, parameters3$par, cv=9, repeats=10)
nine_acc4 <- cross_val(gryphus, parameters4$par, cv=9, repeats=10)
nine_acc5 <- cross_val(gryphus, parameters5$par, cv=9, repeats=10)
nine_acc6 <- cross_val(gryphus, parameters6$par, cv=9, repeats=10)
```
# 3-fold Cross Validation (averaged over 10 repeats)
```{r}
three_avg = c(mean(three_acc1), mean(three_acc2), mean(three_acc3), mean(three_acc4), mean(three_acc5), mean(three_acc6))

for (i in 1:6) {
  cat("Average for P=",i,":", three_avg[i], "\n")
}

```

# 7-fold Cross Validation (averaged over 10 repeats)
```{r}
seven_avg = c(mean(seven_acc1), mean(seven_acc2), mean(seven_acc3), mean(seven_acc4), mean(seven_acc5), mean(seven_acc6))

for (i in 1:6) {
  cat("Average for P=",i,":", seven_avg[i], "\n")
}
```
# 9-fold Cross Validation (averaged over 10 repeats)
```{r}
nine_avg = c(mean(nine_acc1), mean(nine_acc2), mean(nine_acc3), mean(nine_acc4), mean(nine_acc5), mean(nine_acc6))

for (i in 1:6) {
  cat("Average for P=",i,":", nine_avg[i], "\n")
}
```

#Cross-validation graphs
```{r}
P_vals <- c(1,2,3,4,5,6)

cross_val_df = data.frame(P_vals, seven_avg, nine_avg, three_avg)
ggplot(cross_val_df) + 
  geom_point(aes(x= P_vals, y=seven_avg, colour="K=7")) + 
  geom_line(aes(x= P_vals, y=seven_avg, colour="K=7"), linetype="dotted") +
  geom_point(aes(x= P_vals, y=nine_avg, colour="K=9")) + 
  geom_line(aes(x= P_vals, y=nine_avg, colour="K=9"), linetype="dotted") +
  geom_point(aes(x= P_vals, y=three_avg, colour="K=3")) + 
  geom_line(aes(x= P_vals, y=three_avg, colour="K=3"), linetype="dotted") +
  scale_x_continuous(labels=as.character(P_vals),breaks=P_vals) +
    labs(
        x = "Value of P",
        y = "Average Accuracy",
        color = "No. folds (K)",
        title = "K-fold Cross Validation",
        subtitle = "Average accuracy calculated over 10 repeats for K-folds",
        #caption = "caption"
    )
```
```{r}
overfitting_graph <-function(data, params0,cv,title) {
  set.seed(9)
  len <- floor(nrow(data)/cv)
  idxs <- sample(nrow(data), nrow(data))
  val_idxs <- idxs[(len*(0)):(len*1)]
  train_idxs <- idxs[-c((len*(0)):(len*1))]
  res <- optim_poly_logi(data[train_idxs,], params0)
  
  test_pred <- calculate_p(res$par, data[val_idxs,])>=0.5
  acc <- sum(test_pred==data[val_idxs,"surv"])/length(test_pred)
  
  dens <- seq(from=8, to=22.5, by=0.01)
  x.df <- data.frame(dens)
  pred <- calculate_p(res$par, x.df) #preds using the model trained on the test data 
  line.df <- data.frame(dens, pred)
  ggplot() + geom_line(data = line.df, aes(x = dens, y = pred))  + geom_point(aes(x = data$dens[train_idxs], y= data$surv[train_idxs], color = "Training Set")) + geom_point(aes(x = data$dens[val_idxs], y= data$surv[val_idxs], color="Testing Set")) +
    labs(
        x = expression("Density of Lemmings (No. of Lemmings per km"^2*")"),
        y = "Predicted Probability of Survival",
        color = "",
        title = title,
        subtitle = paste("Accuracy for this fold is", round(acc, digits=2))
        #caption = "caption"
    ) + 
    geom_hline(aes(yintercept=.5), color="gray", linetype="longdash") + xlim(8, 22.5) +
    scale_color_manual(values = c("#3D426B", "#b19cd9"))
  
  
}

overfitting_graph(gryphus,parameters5$par,cv=9,title="9-Fold Cross-Validation (P=5)")
overfitting_graph(gryphus,parameters1$par,cv=9,title="9-Fold Cross-Validation (P=1)")
```
##Hypothesis Testing using the likelihood ratio test
We want to use the likelihood ratio test to get us a p value to tell us which models to select. The models we are looking at are all nested (we are trying to decide P i.e. trying to decide for what i beta_j = 0 for j >=i). 
Because the models are nested we can test hypotheses using the GLRT, for example we may want to know what model for eta is better, model 1: eta =  beta_0 + beta_1*(dens - 15)^1 or model 2: eta =  beta_0 + beta_1*(dens - 15)^1 + beta_2*(dens - 15)^2. 
This is testing the hypotheses H_0: beta_2 = 0 versus H_1: beta_2 !=0 (and beta_i = 0 for i>2).

If we wanted to test hypotheses such as does beta_2 = 1, we would fix the paramater beta_2 = 1 and optimise the other parameters. To do this we would use the functions calculate_p_H, ll_poly_logistic_H and optim_poly_logi_H below (theta2 are fixed and its only theta1 paramters that are optimised). 
However since we want to test whether to include some beta_i's in the model for eta (so testing e.g. if beta_2 =0), this is just the same as calculating the neg log likelihood and paramters using the normal calculate_p function twice, once to find the log likelihood under the model eta =  beta_0 + beta_1*(dens - 15)^1 (this would be H_0 : beta_2 = 0) and another time using the model eta =  beta_0 + beta_1*(dens - 15)^1 + beta_2*(dens - 15)^2 (H_1: beta_2 != 0). 

So there is no need to use the functions calculate_p_H, ll_poly_logistic_H and optim_poly_logi_H, but I left them there just in case. There is an example below to show this. 

Hypothesis testing: need to calculate the is the probability (p), under the null hypothesis, of obtaining a likelihood ratio at least as large as that observed.
same as functions in Q1, except the parameters in theta2 are fixed and its only theta1 paramters that are optimised. This can be used for hypothesis testing.  

```{r}
p_val <-function(statement, log_lik_restricted, log_lik_unrestricted, dof) {
  lambda <- 2*(log_lik_unrestricted - log_lik_restricted)
  p <- pchisq(lambda, df = dof, lower.tail = FALSE)
  #reject <- p < 0.05

  print(paste0("There is a p value of ", p))
  if (0.05 < p & p <= 0.1) {
    print(paste("There is marginal evidence to reject the null hypothesis when", statement))
  }
  else if (0.05 >= p & p > 0.01) {
    print(paste("There is evidence to reject the null hypothesis when", statement))
  }
  else if (0.01 >= p & p > 0.001) {
    print(paste("There is strong evidence to reject the null hypothesis when", statement))
  }
  else if (p <= 0.001) {
    print(paste("There is very strong evidence to reject the null hypothesis when", statement))
  }
  else {print(paste("There is no evidence to reject the null hypothesis when", statement))
    }
  #attr(p,"reject") <- reject
}
```

```{r}
#the nested model is always the null (it is restricted) e.g. 
#H_0: beta_2 = 0 (P=1 - nested/restricted model model) versus H_1: beta_2 != 0 (P=2 - more complex/unrestricted model)

#testing P=1 vs P=6
p_val(statement = "testing P=1 vs P=6",log_lik_restricted = -parameters1$value, log_lik_unrestricted = -parameters6$value , dof = 5)
#testing P=1 vs P=5
p_val(statement = "testing P=1 vs P=5",log_lik_restricted = -parameters1$value, log_lik_unrestricted = -parameters5$value , dof = 4)
#testing P=1 vs P=4
p_val(statement = "testing P=1 vs P=4",log_lik_restricted = -parameters1$value, log_lik_unrestricted = -parameters4$value , dof = 3)
#testing P=1 vs P=3
p_val(statement = "testing P=1 vs P=3",log_lik_restricted = -parameters1$value, log_lik_unrestricted = -parameters3$value , dof = 2)
#testing P=1 vs P=2
p_val(statement = "testing P=1 vs P=2",log_lik_restricted = -parameters1$value, log_lik_unrestricted = -parameters2$value , dof = 1)


```
```{r}
#testing P=2 vs P=6
p_val(statement = "testing P=2 vs P=6",log_lik_restricted = -parameters2$value, log_lik_unrestricted = -parameters6$value , dof = 4)
#testing P=2 vs P=5
p_val(statement = "testing P=2 vs P=5",log_lik_restricted = -parameters2$value, log_lik_unrestricted = -parameters5$value , dof = 3)
#testing P=2 vs P=4
p_val(statement = "testing P=2 vs P=4",log_lik_restricted = -parameters2$value, log_lik_unrestricted = -parameters4$value , dof = 2)
#testing P=2 vs P=3
p_val(statement = "testing P=2 vs P=3",log_lik_restricted = -parameters2$value, log_lik_unrestricted = -parameters3$value , dof = 1)
```

```{r}
#testing P=3 vs P=6
p_val(statement = "testing P=3 vs P=6",log_lik_restricted = -parameters3$value, log_lik_unrestricted = -parameters6$value , dof = 3)
#testing P=3 vs P=5
p_val(statement = "testing P=3 vs P=5",log_lik_restricted = -parameters3$value, log_lik_unrestricted = -parameters5$value , dof = 2)
#testing P=3 vs P=4
p_val(statement = "testing P=3 vs P=4",log_lik_restricted = -parameters3$value, log_lik_unrestricted = -parameters4$value , dof = 1)
```

```{r}
#testing P=4 vs P=6
p_val(statement = "testing P=4 vs P=6",log_lik_restricted = -parameters4$value, log_lik_unrestricted = -parameters6$value , dof = 2)
#testing P=4 vs P=5
p_val(statement = "testing P=4 vs P=5",log_lik_restricted = -parameters4$value, log_lik_unrestricted = -parameters5$value , dof = 1)
```
```{r}
#testing P=5 vs P=6
p_val(statement = "testing P=5 vs P=6",log_lik_restricted = -parameters5$value, log_lik_unrestricted = -parameters6$value , dof = 1)
```

The Wilkâ€™s likelihood ratio test in effect penalizes you for the number of extra parameters you are fitting for (that k in Eqn 1 above).  The higher k is, the lower the best-fit negative log-likelihood for the more complex model has to be in order for the null model to be rejected.

## Q5)
Idea is to approximate the sampling distribution of the parameters, sample from
the approximate sampling distribution of the parameter, calculate the function
value for each draw, and then trim alpha/2 from each tail of the distribution of the function values.

Note; above was copied from an article so don't directly copy into final doc.

How to approximate the sampling distribution:
1. asymptotic distribution of MLE
2. boostrap resampling

Below we use the asymptotic distribution of the MLE.

```{r}
ll_gamma <- function(theta,df){
  # Uses a log-transformation to allow unconstrained optimisation as a and b must be >= 0
  alpha <- exp(theta[1])
  s <- exp(theta[2])
  
  # Sets initital value of log-likelihood to zero
  L <- 0
  # Iterates through each row adding the probability of the outcome (0 or 1) to the log-likelihood
  for (i in 1:(nrow(df))){
    # Computes and stores the cdf of a Gamma(alpha,s) distribution
    p <- pgamma(df[i,1], shape = alpha, scale = s)
    if (is.nan(p)){
      #cat(alpha,s)
      return(9999)
    }
    # If surv = 1 then the proability of this occuring was p, and the probability of surv = 0 is (1-p)
    if (df[i,2] == 1){
      L <- L + log(p)
    } else {
      L <- L + log(1-p)
    }
  }
  -L
}

# Initial values using the expectation and variance values for a gamma distribution.
alpha0 <- mean(gryphus$dens)^2/var(gryphus$dens)
s0 <- var(gryphus$dens)/mean(gryphus$dens)

theta1 <- optim(ll_gamma,par=c(log(alpha0),log(s0)),df=gryphus,method="BFGS")$par
alpha <- exp(theta1[1])
s <- exp(theta1[2])
cat("alpha =",alpha," s =",s)
```



```{r}
plot_CI_gamma <- function(df,n,lower,upper,steps){
  
  fit <- optim(ll_gamma,par=c(log(alpha0),log(s0)),df=gryphus,hessian = TRUE,method="BFGS")
  theta.ml <- (fit$par)
  I <- (solve(fit$hessian))
  theta_samp <- mvrnorm(n,theta.ml,I)

  x <- seq(lower,upper,(upper - lower)/steps)
 
  theta_samp <- exp(theta_samp)

  p_sims <- sapply(x,pgamma,shape = theta_samp[,1],scale = theta_samp[,2])
    
  
  alpha <- exp(theta.ml[1])
  s <- exp(theta.ml[2])
  p_mle <- pgamma(x,shape = alpha,scale = s)
  
  bounds <- matrix(0,ncol(p_sims),2)
  
  #print(as.data.frame(p_sims))
  
 for (i in 1:ncol(p_sims)){
    bounds[i,] <- (quantile(p_sims[,i],c(0.025,0.975),names = FALSE))
 }

  
  CI.df <- data.frame("dens" = x, "li" = bounds[,1], "mle" = p_mle,"ui" = bounds[,2])
  
  print(CI.df)
  
  
  ggplot() + geom_point(data = gryphus, aes(x = dens, y = surv, color = factor(surv))) + geom_line(data = CI.df, aes(x = dens, y = mle))+ geom_line(data = CI.df, aes(x = dens, y = li), color = "grey70") + geom_line(data = CI.df, aes(x = dens, y = ui), color = "grey70") +
    geom_ribbon(data = CI.df, aes(ymin = li, ymax = ui, x= dens), fill = "grey70", alpha = 0.3) + 
    labs(
        x = "Density of lemmings (number-of-lemmings/km2)",
        y = "Predicted Probability of Survival",
        color = "Survived",
        title = "title",
        #subtitle = "subtitle",
        #caption = "caption"
    )
  
  
  
}

plot_CI_gamma(gryphus,1000,0,25,1000)
```


```{r}
plot_comparison <- function(df,pars = parameters1,n,lower,upper,steps){
  x <- seq(lower,upper,(upper - lower)/steps)
  
  fit <- optim(ll_gamma,par=c(log(alpha0),log(s0)),df=gryphus,hessian = TRUE,method="BFGS")
  theta.ml <- (fit$par)
  
  alpha <- exp(theta.ml[1])
  s <- exp(theta.ml[2])
  gamma_mle <- pgamma(x,shape = alpha,scale = s)
  
  probs1 <- calculate_p(pars$par, data.frame("dens" = x))
  
  
  
  df <- data.frame("dens" = x, "gamma" = gamma_mle, "logit" = probs1)
  
  ggplot() + geom_point(data = gryphus, aes(x = dens, y = surv, color = factor(surv))) + geom_line(data = df, aes(x = dens, y = gamma), color = "magenta1") + geom_line(data = df, aes(x = dens, y = logit),color = "green4") +
    labs(
        x = "Density of lemmings (number-of-lemmings/km2)",
        y = "Predicted Probability of Survival",
        color = "Survived",
        color = "Model",
        title = "title",
        #subtitle = "subtitle",
        #caption = "caption"
    )
  
}

plot_comparison(gryphus, parameters1,120,8,22,100)
plot_comparison(gryphus, parameters2,120,8,22,100)
```
