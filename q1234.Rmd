
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
load(file = url("https://mfasiolo.github.io/TOI/gryphus2.RData"))
(gryphus)

library("gamlss.dist")
library(caret)
library(ggplot2)
```

# Q1)

```{r}
calculate_p <- function(theta, df){
  P <- length(theta) - 1
  heta <- rep(theta[1], nrow(df))
  for(degree in 1:P){
    heta <- heta + theta[degree+1]*(df$dens - 15)^degree
  }
  
  p <- exp(heta)/(exp(heta)+1)
  # in case inf/inf -> NaN
  p[is.nan(p)] <- 1
  return(p)
}

ll_poly_logistic <- function(theta, df){
  
  P <- length(theta) - 1
  
  # Sets initial value of log-likelihood to zero
  L <- 0
  
  p <- calculate_p(theta, df)
  
  # Iterates through each row adding the probability of the outcome (0 or 1) to the log-likelihood
  for (i in 1:nrow(df)){
    if (df[i,2] == 1){
      L <- L + log(p[i] + 1e-6)
    } else {
      L <- L + log(1-p[i] + 1e-6)
    }
  }
  
  return(-L)
}
```

# Q2)

```{r}
optim_poly_logi <- function(df, theta0){
  res <- optim(
    ll_poly_logistic,
    par=theta0,
    df=df,
    method="BFGS",
    hessian=T,
    control=list(maxit=500)
  )
  if(res$convergence == 0){
    return(res)
  }else{
    print("Error!")
    return()
  }
}
```



```{r}
# MLE with full data result
# P = 1
theta0 <- c(0, 0)
parameters1 <- optim_poly_logi(gryphus, theta0)
# P = 2
parameters2 <- optim_poly_logi(gryphus, c(parameters1$par, 0))
# P = 3
parameters3 <- optim_poly_logi(gryphus, c(parameters2$par, 0))
# P = 4
parameters4 <- optim_poly_logi(gryphus, c(parameters3$par, 0))
# P = 5
parameters5 <- optim_poly_logi(gryphus, c(parameters4$par, 0))
# P = 6
parameters6 <- optim_poly_logi(gryphus, c(parameters5$par, 0))
# P = 7
parameters7 <- optim_poly_logi(gryphus, c(parameters6$par, 0))
#test
#gryphus$dens2 <- gryphus$dens - 15
#mod <- glm(surv ~ dens2, data = gryphus, family = binomial)
```

```{r}
pred1 <- calculate_p(parameters1$par, gryphus)
pred2 <- calculate_p(parameters2$par, gryphus)
pred3 <- calculate_p(parameters3$par, gryphus)
pred4 <- calculate_p(parameters4$par, gryphus)
pred5 <- calculate_p(parameters5$par, gryphus)
pred6 <- calculate_p(parameters6$par, gryphus)

data <- cbind(gryphus, pred1, pred2, pred3, pred4, pred5, pred6)

data <- rbind(gryphus, gryphus, gryphus, gryphus, gryphus, gryphus)
preds <- c(pred1, pred2, pred3, pred4, pred5, pred6)
data <- cbind(data, preds)
data$cat <- as.factor(sort(rep(1:6, nrow(gryphus))))

ggplot(data) +
  geom_boxplot(aes(group=cat, y=preds))
```

# Q3)

```{r}
#confidence intervals from George
plot_p_ci <- function(pars, df){
  pred <- calculate_p(pars$par, df)
  X <- rep(1, nrow(df))
  for(i in 1:(length(pars$par)-1)){
    X <- cbind(X, (df$dens-15)^i)
  }
  Sigma <- solve(pars$hessian)
  est_var <- diag(X%*%Sigma%*%t(X))
  est_heta <- cbind(X%*%pars$par - 1.96*sqrt(est_var),
                X%*%pars$par + 1.96*sqrt(est_var))
  est_ci <- exp(est_heta)/(1+exp(est_heta))
  
  data <- cbind(gryphus, pred, est_ci)
  colnames(data)[4:5] <- c("li", "ui")
  ggplot(data) +
    geom_point(aes(dens, surv)) +
    geom_point(aes(dens, pred), color="red") + 
    geom_errorbar(
      aes(x=dens, y=pred, ymin=li, ymax=ui), colour="orange",
      width=.1, position=position_dodge(0.1)
    ) +
    geom_hline(aes(yintercept=.5), linetype="longdash")
}
#plot_p_ci(parameters3, gryphus)
```

```{r}
plot_CI <-function(pars, df, title) {
  pred <- calculate_p(pars$par, df)
  scatter.df <- data.frame(df, pred)
  
  dens <- seq(from=8, to=23, by=0.01)
  x.df <- data.frame(dens)
  probs1 <- calculate_p(pars$par, x.df)
  line.df <- data.frame(dens, probs1)

  X <- rep(1, nrow(x.df))
  for(i in 1:(length(pars$par)-1)){
    X <- cbind(X, (dens-15)^i)
  }
  sigma <- solve(pars$hessian)
  est_var <- diag(X%*%sigma%*%t(X))
  est_heta <- cbind(X%*%pars$par - 1.96*sqrt(est_var),
                X%*%pars$par + 1.96*sqrt(est_var))
  est_ci <- exp(est_heta)/(1+exp(est_heta))
  
  li <- est_ci[,1]
  ui <- est_ci[,2]
  CI.df <- data.frame(x.df$dens, li, ui)

  ggplot() + geom_point(data = scatter.df, aes(x = dens, y = pred, color = factor(surv))) + geom_line(data = line.df, aes(x = dens, y = probs1))+ geom_line(data = CI.df, aes(x = x.df.dens, y = li), color = "grey70") + geom_line(data = CI.df, aes(x = x.df.dens, y = ui), color = "grey70") +
    geom_ribbon(data = CI.df, aes(ymin = li, ymax = ui, x= x.df.dens), fill = "grey70", alpha = 0.3) + 
    labs(
        x = "Density of lemmings (number-of-lemmings/km2)",
        y = "Predicted Probability of Survival",
        color = "Survived",
        title = title,
        #subtitle = "subtitle",
        #caption = "caption"
    ) + 
    geom_hline(aes(yintercept=.5), color="gray", linetype="longdash") + xlim(8, 23)
}
```

```{r}
plot_CI(parameters1, gryphus, title = "When P=1")
plot_CI(parameters2, gryphus, title = "When P=2")
plot_CI(parameters3, gryphus, title = "When P=3")
plot_CI(parameters4, gryphus, title = "When P=4")
plot_CI(parameters5, gryphus, title = "When P=5")
plot_CI(parameters6, gryphus, title = "When P=6")
#TO DO: fix warnings, P=4,5 look weird change that, make it go to the end of axis 
```
```{r}
plot_all <-function() {
  dens <- seq(from=8, to=23, by=0.01)
  x.df <- data.frame(dens)
  
  probs1 <- calculate_p(parameters1$par, x.df)
  probs2 <- calculate_p(parameters2$par, x.df)
  probs3 <- calculate_p(parameters3$par, x.df)
  probs4 <- calculate_p(parameters4$par, x.df)
  probs5 <- calculate_p(parameters5$par, x.df)
  probs6 <- calculate_p(parameters6$par, x.df)
  
  line.df <- data.frame(dens, probs1, probs2, probs3, probs4, probs5, probs6)
  
  ggplot() + geom_line(data = line.df, aes(x = dens, y = probs1, color= "P=1")) + geom_line(data = line.df, aes(x = dens, y = probs2, color= "P=2")) + geom_line(data = line.df, aes(x = dens, y = probs3, color= "P=3")) + geom_line(data = line.df, aes(x = dens, y = probs4, color= "P=4")) + geom_line(data = line.df, aes(x = dens, y = probs5, color= "P=5")) + geom_line(data = line.df, aes(x = dens, y = probs6, color= "P=6")) + 
    labs(
        x = "Density of lemmings (number-of-lemmings/km2)",
        y = "Predicted Probability of Survival",
        color = "Model",
        title = "Overlapping Models",
        #subtitle = "subtitle",
        #caption = "caption"
    ) + 
    geom_hline(aes(yintercept=.5), color="gray", linetype="longdash") + xlim(8, 23)
  
}

plot_all()
```
#Q4) 

Just by looking at the graphs of the different models we can probabably rule out P=2 as the model begins to slope downwards when the density gets to around a desnity of 21. This is unlikely given the context of the problem. The P=4 model also suddenly slopes downwards but this is at a density of 22 and after which we don't have data for so it is unlikely to be inaccurate. Overall, there is not much data to work with and it is difficult to tell the value of P but just looking at the visuals. 

##Which model has the highest accuracy (what predicts the survival correct most of the time)? Not sure if this is relevant but seemed like the most natural thing to do. 
```{r}
accuracy <-function(pred, df) {
  surv_pred <- round(pred)
  correct <- surv_pred == df$surv
  acc <-sum(correct, na.rm = TRUE) / length(correct)
  return(acc)
}

accuracy(pred1,gryphus)
accuracy(pred2,gryphus)
accuracy(pred3,gryphus)
accuracy(pred4,gryphus)
accuracy(pred5,gryphus)
accuracy(pred6,gryphus)
```
Overall all accuracies  similiar but for some reason P=2 does slightly worse. P=5 and P=6 performed the best, this is likely due to overfitting. 

##Hypothesis Testing using the likelihood ratio test
We want to use the likelihood ratio test to get us a p value to tell us which models to select. The models we are looking at are all nested (we are trying to decide P i.e. trying to decide for what i beta_j = 0 for j >=i). 
Because the models are nested we can test hypotheses using the GLRT, for example we may want to know what model for eta is better, model 1: eta =  beta_0 + beta_1*(dens - 15)^1 or model 2: eta =  beta_0 + beta_1*(dens - 15)^1 + beta_2*(dens - 15)^2. 
This is testing the hypotheses H_0: beta_2 = 0 versus H_1: beta_2 !=0 (and beta_i = 0 for i>2).

If we wanted to test hypotheses such as does beta_2 = 1, we would fix the paramater beta_2 = 1 and optimise the other parameters. To do this we would use the functions calculate_p_H, ll_poly_logistic_H and optim_poly_logi_H below (theta2 are fixed and its only theta1 paramters that are optimised). 
However since we want to test whether to include some beta_i's in the model for eta (so testing e.g. if beta_2 =0), this is just the same as calculating the neg log likelihood and paramters using the normal calculate_p function twice, once to find the log likelihood under the model eta =  beta_0 + beta_1*(dens - 15)^1 (this would be H_0 : beta_2 = 0) and another time using the model eta =  beta_0 + beta_1*(dens - 15)^1 + beta_2*(dens - 15)^2 (H_1: beta_2 != 0). 

So there is no need to use the functions calculate_p_H, ll_poly_logistic_H and optim_poly_logi_H, but I left them there just in case. There is an example below to show this. 
```{r}

#Hypothesis testing: need to calculate the is the probability (p), under the null hypothesis, of obtaining a likelihood ratio at least as large as that observed.

#same as functions in Q1, except the parameters in theta2 are fixed and its only theta1 paramters that are optimised. This can be used for hypothesis testing.  
calculate_p_H<- function(theta1, df, theta2){
  theta = c(theta1, theta2)
  P <- length(theta) - 1
  heta <- rep(theta[1], nrow(df))
  for(degree in 1:P){
    heta <- heta + theta[degree+1]*(df$dens - 15)^degree
  }
  
  p <- exp(heta)/(exp(heta)+1)
  # in case inf/inf -> NaN
  p[is.nan(p)] <- 1
  
  return(p)
}

ll_poly_logistic_H <- function(theta1, df,theta2){
  theta = c(theta1, theta2)
  P <- length(theta) - 1
  
  # Sets initial value of log-likelihood to zero
  L <- 0
  
  p <- calculate_p_H(theta1,df,theta2)
  
  # Iterates through each row adding the probability of the outcome (0 or 1) to the log-likelihood
  for (i in 1:nrow(df)){
    if (df[i,2] == 1){
      L <- L + log(p[i] + 1e-6)
    } else {
      L <- L + log(1-p[i] + 1e-6)
    }
  }
  
  return(-L)
}

optim_poly_logi_H <- function(df, theta1,theta2){
  res <- optim(
    ll_poly_logistic_H,
    par=theta1,
    df=df,
    theta2 = theta2,
    method="BFGS",
    hessian=T,
    control=list(maxit=500)
  )
  if(res$convergence == 0){
    return(res)
  }else{
    print("Error!")
    return()
  }
}

#beta_5 is fixed to 0. So this is used to test if beta_5 =0 (which means beta_6 =0, beta7 = 0 etc.)
parameters4H <- optim_poly_logi_H(theta1 = c(parameters3$par,0), gryphus, theta2 = 0)
parameters4H$par
parameters4H$value
#these are the same results we got when we used the model with P=4 to find our MLE
parameters4$par
parameters4$value
```

```{r}
p_val <-function(statement, log_lik_restricted, log_lik_unrestricted, dof) {
  lambda <- 2*(log_lik_unrestricted - log_lik_restricted)
  p <- pchisq(lambda, df = dof, lower.tail = FALSE)
  #reject <- p < 0.05

  print(paste0("There is a p value of ", p))
  if (0.05 < p & p <= 0.1) {
    print(paste("There is marginal evidence to reject the null hypothesis when", statement))
  }
  else if (0.05 >= p & p > 0.01) {
    print(paste("There is evidence to reject the null hypothesis when", statement))
  }
  else if (0.01 >= p & p > 0.001) {
    print(paste("There is strong evidence to reject the null hypothesis when", statement))
  }
  else if (p <= 0.001) {
    print(paste("There is very strong evidence to reject the null hypothesis when", statement))
  }
  else {print(paste("There is no evidence to reject the null hypothesis when", statement))
    }
  #attr(p,"reject") <- reject
}
```

```{r}
#the nested model is always the null (it is restricted) e.g. 
#H_0: beta_2 = 0 (P=1 - nested/restricted model model) versus H_1: beta_2 != 0 (P=2 - more complex/unrestricted model)

#testing P=1 vs P=6
p_val(statement = "testing P=1 vs P=6",log_lik_restricted = -parameters1$value, log_lik_unrestricted = -parameters6$value , dof = 5)
#testing P=1 vs P=5
p_val(statement = "testing P=1 vs P=5",log_lik_restricted = -parameters1$value, log_lik_unrestricted = -parameters5$value , dof = 4)
#testing P=1 vs P=4
p_val(statement = "testing P=1 vs P=4",log_lik_restricted = -parameters1$value, log_lik_unrestricted = -parameters4$value , dof = 3)
#testing P=1 vs P=3
p_val(statement = "testing P=1 vs P=3",log_lik_restricted = -parameters1$value, log_lik_unrestricted = -parameters3$value , dof = 2)
#testing P=1 vs P=2
p_val(statement = "testing P=1 vs P=2",log_lik_restricted = -parameters1$value, log_lik_unrestricted = -parameters2$value , dof = 1)


```
```{r}

#testing P=2 vs P=6
p_val(statement = "testing P=2 vs P=6",log_lik_restricted = -parameters2$value, log_lik_unrestricted = -parameters6$value , dof = 4)
#testing P=1 vs P=5
p_val(statement = "testing P=2 vs P=5",log_lik_restricted = -parameters2$value, log_lik_unrestricted = -parameters5$value , dof = 3)
#testing P=1 vs P=4
p_val(statement = "testing P=2 vs P=4",log_lik_restricted = -parameters2$value, log_lik_unrestricted = -parameters4$value , dof = 2)
#testing P=1 vs P=3
p_val(statement = "testing P=2 vs P=3",log_lik_restricted = -parameters2$value, log_lik_unrestricted = -parameters3$value , dof = 1)
```
```{r}
#testing P=3 vs P=6
p_val(statement = "testing P=3 vs P=6",log_lik_restricted = -parameters3$value, log_lik_unrestricted = -parameters6$value , dof = 3)
#testing P=3 vs P=5
p_val(statement = "testing P=3 vs P=5",log_lik_restricted = -parameters3$value, log_lik_unrestricted = -parameters5$value , dof = 2)
#testing P=3 vs P=4
p_val(statement = "testing P=3 vs P=4",log_lik_restricted = -parameters3$value, log_lik_unrestricted = -parameters4$value , dof = 1)
```
```{r}
#testing P=4 vs P=6
p_val(statement = "testing P=4 vs P=6",log_lik_restricted = -parameters4$value, log_lik_unrestricted = -parameters6$value , dof = 2)
#testing P=4 vs P=5
p_val(statement = "testing P=4 vs P=5",log_lik_restricted = -parameters4$value, log_lik_unrestricted = -parameters5$value , dof = 1)
```
```{r}
#testing P=5 vs P=6
p_val(statement = "testing P=4 vs P=6",log_lik_restricted = -parameters5$value, log_lik_unrestricted = -parameters6$value , dof = 1)
```

This is from online....
Usually more complex models fit the data better than a simpler nested model. This is because the more complex model has more parameters, and those extra parameters give the model more wiggle room to fit to variations in the data, this is known as overfitting. But the problem is that more parameters also increases the uncertainty on all the parameter estimates; so, yes… it might give a lower negative log-likelihood, but there is a cost to be paid for that in the number of parameters you had to add in. The more parameters you add in, the more you are at risk of “over-fitting” to statistical fluctuations in the data, rather than trends due to true underlying dynamics.

The Wilk’s likelihood ratio test in effect penalizes you for the number of extra parameters you are fitting for (that k in Eqn 1 above).  The higher k is, the lower the best-fit negative log-likelihood for the more complex model has to be in order for the null model to be rejected.

```{r}
#unsure of what other 'appropriate statistical tests or model selection criteria' I should do. Maybe AIC.
```

